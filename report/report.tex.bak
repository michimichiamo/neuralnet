\documentclass[11pt,a4paper]{article}
\author{Michele Iannello}
\date{\today}
\title{Exploiting OpenMP and CUDA parallelization to enable efficient Neural Network evaluation}
\usepackage[centering, total={8in, 9in}]{geometry}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{amsmath}
\numberwithin{equation}{section}
\usepackage{textcomp}
%\pagestyle{plain}
\textwidth=450pt\oddsidemargin=0pt
\usepackage{caption}
\captionsetup{format=hang, labelfont={bf}, figureposition=bottom, font=small}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.png}
\usepackage[pdfpagelabels]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=blue,
    citecolor=black
}
\urlstyle{same}

\begin{document}
\selectlanguage{english}
\hypersetup{pageanchor=false}
\maketitle
\begin{abstract}
This report describes two different C code implementations - the former making use of the \textbf{OpenMP}\cite{OpenMP:2013} API and the latter of \textbf{NVIDIA CUDA}\cite{CUDA:2007} - of a simple \textit{feed-forward sparsely-connected multi-layer} \textbf{Neural Network}. In particular, the aim of the presented work is to exploit parallelization techniques in order to allow an efficient evaluation of the network and to evaluate the performances through the assessment of different metrics. Results are presented by means of plots realized using Python module \textbf{Matplotlib}\cite{Hunter:2007}.
\end{abstract}

\section*{Outline}
This project was developed as part of the final assessment for the course \textit{Architecture and Platforms for Artificial Intelligence} within the Master Course in Artificial Intelligence at the University of Bologna.
The subject of the project is a feed-forward sparsely-connected multi-layer Neural Network whose structure is described in the assignment document.
The development of the work was performed exploiting the \textit{lab machine}, a GPU-equipped computer located at the University and made (remotely) available to the course students thanks to the course professor.
\subsection*{Executable files}
It is useful to firstly provide some brief instructions, in order to correctly run the executable files\footnote{We will refer to the files either as \textit{Program 1/2} or \textit{CPU/GPU implementation}} obtained by compiling the source files \texttt{nn\_omp.c} and \texttt{nn\_cuda.cu} according to the specifications contained both within the \texttt{README.md} file and the source files themselves.
Both programs accept two command line arguments: the size of the input for the first layer ($N$) and the number of layers ($K$) of the network; if no argument is provided, they default to $N = 500000$ and $K = 150$ (which were observed to generate data so as to approach -- but not exceed -- the memory limit of about 3000 MB of the \textit{lab machine} GPU).
At the execution of both programs, the \textbf{execution time} is printed to the console. This measure refers to the actual \textit{computation} time -- not taking e.g. memory allocation and data transfer into account -- and is computed exploiting the function \texttt{hpc\_gettime()} contained in the header file \textit{hpc.h}, provided by the professor.

\subsection*{Data generation}
The constant \texttt{R=5} is \texttt{\#define}d at the beginning of both source programs, i.e. it is a \textit{compile-time} constant. In the second program, the constant \texttt{BLKDIM=1024} is \texttt{\#define}d as well.
Input data is randomly generated at run-time, right before the evaluation of the network, and it is organized in $K$ \texttt{structs}, each storing inputs (only for the CPU implementation\footnote{In the GPU implementation, an array was used to store inputs (and was overwritten for each layer) in order to lower the amount of required GPU memory.}), weights and bias for a different layer.
This choice makes the code clearer and easier to read, along with allowing a simpler management of the input data: since each layer has a different number of (inputs and) weights, storing data in few big arrays (instead of many \texttt{structs}) would result in non-trivial indexing in order to retrieve the corresponding data, requiring further computations.

\subsection*{Functions}
The \textit{forward pass} of the network, i.e. the actual computation of the activations, is performed by the repeated call of the function \texttt{kernel()}, which is invoked $K$ times inside the \texttt{forward()} function. At each iteration, the latter takes care of storing the activations -- computed by the former -- as input for the following layer (namely in the corresponding \texttt{struct} for Program 1).
Inside \texttt{kernel()}, a nested loop goes firtly through output neurons, initializes each activation to the bias value $b_{k}$ and then repeatedly updates it -- through MAC operations -- looping over the corresponding $R$ values of the input neurons.

\section{CPU implementation}
\label{sec:omp}
\subsection{Parallelization techniques}
First of all, some considerations about the problem need to be done in order to account for the deployed parallelization techniques.
\begin{itemize}
	\item Each layer depends on the previous one, since activations of layer $k-1$ serve as input for layer $k$. This means that parallelizing over \texttt{kernel()} calls -- e.g. assigning different layers to different \texttt{tasks} -- is meaningless: layer $k$ needs to be evaluated \textit{strictly after} layer $k-1$.
	\item The compile-time constant $R$ is assumed to be \textit{small}, according to the assignment specifications. As a consequence, we can conclude that parallelizing the loop over the $R$ input values that contribute to each output neuron would not be worthwhile.
\end{itemize}
Taken these considerations into account, we can efficiently \textbf{parallelize} only over \textbf{output neurons} belonging to the same layer, which is achieved through a \texttt{\#pragma omp parallel for} directive inside \texttt{kernel()}.
Essentially, output neurons within a single layer are distributed among threads, each taking care of computing the activations for one neuron at a time.
The clause \texttt{private(i,j)} ensures that the two loop variables are private to each thread, while the clause \texttt{schedule(static)} causes the iterations to be evenly divided contiguously among threads: due to the structure of the network, we know that each iteration requires \textit{exactly} the same number of operations, thus threads have an even workload.

It is worth pointing out that other parallelization techniques could be suitable for this problem -- at least in principle -- and briefly illustrating the reasons why they were not deployed.
The \texttt{collapse} clause would allow to extend the parallelization also to the innermost loop (although with arguably little advantage, as mentioned above), but it would require to move the initialization and the application of the activation function outside the loops so as to obtain \textit{perfectly nested loops}, which would result in greater effort overall.
Likewise, although it could seem very appropriate to use the \texttt{reduction} clause, the core computation of the network being the MAC operation, this would require to instantiate \textit{private} temporary variables to store activations -- since the clause does not accept array elements as accumulation variables -- producing a copy latency that would end up worsening the performance instead of improving it.

\subsection{Performance evaluation}
The performance evaluation for the first program was carried out on the above mentioned \textit{lab machine} exploiting an Intel\textsuperscript{\tiny\textregistered} Core\textsuperscript{TM} i7-2600 CPU working at 3.40GHz -- which has 8 logical cores available -- and determined according to 3 different metrics:
\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.45\textwidth]{omp-speedup.png}
\includegraphics[width=0.45\textwidth]{omp-strong.png}
\caption{\label{fig:omp-speedup-strong} Speedup and Strong scaling efficiency for the CPU implementation using default values $N=500000$ and $K=150$. Execution times employed to compute both metrics result from an average on 5 different runs.}
\end{center}
\end{figure}
\begin{itemize}
	\item \textbf{Speedup} was computed as $S\left(p\right) = T_{parallel}\left(1\right)/{T_{parallel}\left(p\right)}$ for each number of processors $p$, where $T_{parallel}\left(p\right)$ indicates the execution time of the program using $p$ processors. It basically measures the improvement achieved by exploiting a certain number of processors instead of a single one, and its ideal value is $S\left(p\right) = p$.
	\item \textbf{Strong scaling efficiency}, computed as $E\left(p\right)= S\left(p\right)/p$ for each $p$, is a sort of \textit{relative speedup}, i.e. weighted on the number of employed processors, and has a (constant) ideal value of $E\left(p\right)=1$.
	\item \textbf{Weak scaling efficiency} was computed as $W\left(p,K\right)= T_{1}\left(1,K\right)/{T_{p,K}\left(p\right)}$ for each $p$ and for different values of $K$, where $T_{p}$ refers to the execution time spent to complete $p$ \textit{work units}. It aims at measuring performance keeping the per-processor work fixed.
\end{itemize}
Figure \ref{fig:omp-speedup-strong} shows both \textbf{Speedup} and \textbf{Strong scaling efficiency} over the number of exploited processors, obtained by averaging the execution times on 5 different runs (using default values for parameters $N$ and $K$) in order to get a more reliable result.
Inspecting the plots, we note a slightly sub-linear increase in $S\left(p\right)$ until $p=4$, after which it drops and starts rising again with a similar trend; we can observe $E\left(p\right)$ decreasing up to $p=5$ -- with a significant drop in the last step -- to remain roughly constant afterwards.
It is possible to notice how the results of both plots seem to be well divided into two groups of 4 points, which is probably related to the \textit{hyper-threading} technology underneath. Indeed, the first 4 exploited processors are \textit{physical} cores, while the last 4 are \textit{virtual} cores: the latter are only able to exploit the stalls of the former, thus granting a poorer performance and affecting both evaluation metrics.
\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.6\textwidth]{omp-weak.png}
\caption{\label{fig:omp-weak} Weak scaling efficiency for the CPU implementation using values $N_{p}=50000*p$ for each $p$ and $k=10,50,100,150$. Execution times employed to compute it result from an average on 5 different runs.}
\end{center}
\end{figure}

On the other hand, \textbf{Weak scaling efficiency} -- reported in Figure \ref{fig:omp-weak} -- exhibits a peculiar behaviour.
We firstly point out that computing $W\left(p,K\right)$ required several sets of runs, since the total amount of work needs to be proportional to $p$, which was achieved by setting $N_{p} = p*N$ (with very good approximation, given that $R$ is \textit{small}\footnote{Actually, layer $k$ has $N-(k-1)*\left(R-1\right)$ inputs, thus $N_{p}$ is slightly lower than $p*N$}) and fixing $K$ for each set of 5 executions.
The runs with $K=50,100,150$ show a trend resembling that of $E\left(p\right)$: we observe a slow decrease in $W\left(p\right)$ up to $p=4$, a significant drop at $p=5$ and a roughly constant value afterwards, which we could relate to the already mentioned reasons.
However, when it comes to $K=10$ we can observe an unexpected rise in $W\left(p\right)$ up to $p=3$, when it starts decreasing with a trend comparable to the other runs. We could maybe relate this behaviour to the relatively very small number of computations required by a network with just $K=10$ layers, which can produce the CPU to enact memory optimizations such as \textit{caching} techniques, which dramatically improve performance for small numbers of computations.

\section{GPU implementation}
\subsection{Parallelization techniques}
Recalling the observations made at the beginning of Section \ref{sec:omp}, we can know discuss the parallelization techniques used for the second program.
The computation for each layer is performed calling \texttt{kernel$<<<$(n+BLKDIM-1)/(BLKDIM), BLKDIM$>>>$()} -- using CUDA \textit{execution configuration} syntax -- where \texttt{BLKDIM=1024}, i.e. the maximum number of threads per block for the device (which allows, at least in principle, to fully exploit the computational power of each \textit{SM}\footnote{CUDA Streaming Multiprocessor}), and $n$ is the number of output neurons of the layer. Accordingly, each thread computes the activation for a single, different element of the output array.

 Just like before, we could deploy further techniques with respect to this configuration, which however would not improve the performance:
 \begin{itemize}
 	\item \textbf{Parallelizing over the $R$ loop} would allow multiple (precisely $R$) threads to concurrently update a single output value, requiring to handle \textit{race conditions} and to deal with \textit{synchronization overhead}.
 	\item The use of \textbf{shared memory} is usually exploited to improve performance when a certain amount of data needs to be accessed multiple times within the same block -- which in our case happens for bias $b_{k}$ and inputs $x_{1,k},...,x_{n,k}$ of layer $k$. However, in our case it would imply a \textit{synchronization overhead} anyway more costly than the benefits provided by the faster memory access: in the latter case, in particular, each value is reused only $R$ times (which we recall being small), while in the former case the reuse is equal to \texttt{BLKDIM}.\footnote{These techniques were tested and were observed not providing benefits, rather producing a worsening in the performance of the program.}
\end{itemize}

\subsection{Performance evaluation}
The performance evaluation was carried out on the above mentioned \textit{lab machine} exploiting a NVIDIA GeForce\textsuperscript{\tiny\textregistered} GTX 580 GPU running at 1.57 GHz and with a global memory of 3005 MB.
Two different evaluation metrics were employed to assess the performance of the Program 2:
\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.6\textwidth]{cuda-throughput.png}
\caption{\label{fig:cuda-throughput} Throughput of the GPU implementation for different input sizes, with $k=150$. Execution times employed to compute it result from an average on 5 different runs.}
\end{center}
\end{figure}
\begin{itemize}
	\item \textbf{Throughput} was computed as $T\left(N_{processed}\right) = N_{processed}/{T_{GPU}}$, where $NN_{processed}$ is the total number of processed data and $T_{GPU}$ is the execution time of Program 2. It gives information about the amount of data processed in the time unit, and is commonly used based on the fact that GPU parallel programming addresses acceleration aiming at performing \textit{a lot of} computations at the same time, rather than \textit{each single} computation faster.
	\item \textbf{Speedup with respect to the CPU implementation} was computed as $S\left(N\right) = T_{CPU}\left(N\right)/{T_{GPU}\left(N\right)}$, where $N$ is the usual first layer's input size and $T_{CPU}$ refers to the execution time of the CPU implementation \textit{exploiting all available processors}. This allows a fair comparison between the two implementations, avoiding spurious speedups being recorded.
\end{itemize}
\textbf{Throughput} is reported in Figure \ref{fig:cuda-throughput} and exhibits a logarithmic trend in $N$ -- evidenced by setting a logarithmic scale for the x-axis. This highlights that improvement provided by GPU parallelization grows more significantly when the GPU remains mainly unused -- and thus the additional effort for increasing the input size amounts to scheduling more blocks --, and less meaningfully as more \textit{SMs} become gradually active and allow to take more and more advantage of the computational power of the device.

Figure \ref{fig:cuda-speedup} shows \textbf{Speedup with respect to the CPU implementation} over $N$ for fixed $K$. We can notice a dramatic escalation in the first few points, then a drop and a more gentle rise afterwards: this can be explained taking into account that the first 3 execution times for Program 1 grow with $N$, while the corresponding ones for Program 2 are nearly equal to each other -- probably due to the still partial usage of the GPU \textit{SMs}. Indeed, at higher values of $N$ -- which bring along a more significant exploitation of the device -- speedup acquires a more meaningful trend: the GPU implementation exhibits a considerable improvement in performance with respect to the CPU one.

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.6\textwidth]{cuda-speedup.png}
\caption{\label{fig:cuda-speedup} Speedup of the GPU implementation with respect to the CPU implementation for different input sizes, with $k=150$. Execution times for both CPU and GPU implementations result from an average on 5 different runs.}
\end{center}
\end{figure}


\bibliographystyle{unsrt}
\bibliography{report}

\end{document}

%\bibitem{cms_es} \url{http://cms.ciemat.es}
%\bibitem{p-p} CMS Collaboration, \textit{Performance of the CMS muon detector and muon reconstruction with proton-proton collisions at $\sqrt{s} = 13 TeV$}, \href{<https://arxiv.org/pdf/1804.04528v2.pdf>}{arXiv:1804.04528 [physics.ins-det]}
%\bibitem{lumi} Werner Herr and Bruno Muratori, \textit{Concept of luminosity}, Geneva, 2006
%\bibitem{lumi_coll_rate} James Gillies, \textit{Luminosity? Why donâ€™t we just say collision rate?}, \href{<https://www.quantumdiaries.org/2011/03/02/why-don%E2%80%99t-we-just-say-collision-rate/>}{Quantum Diaries}, 2 March 2011
%\bibitem{lhc_full} Rende Steerenberg, \textit{LHC Report: The LHC is full!}, \href{<https://home.cern/news/news/accelerators/lhc-report-lhc-full}{CERN News}, 15 May 2018
%\bibitem{cern} \url{https://home.cern}
%\bibitem{cms} \url{https://cms.cern}
